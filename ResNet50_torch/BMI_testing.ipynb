{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b7bdc0-8932-442f-9bc6-a9237e17c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ec2033-4513-4900-a13d-92e8510254ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import os \n",
    "import glob\n",
    "import random\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "772a2cad-8a22-44ae-b158-d95f4f3242fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b936546d-d559-4a6f-b2fe-585a6fc21a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "# Set your base path where images are stored.\n",
    "image_base_path = \"BMI/Data/Images\"  # update this to your actual image directory\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Create a new column 'file_path' with the full path for each image.\n",
    "df['file_path'] = df['name'].apply(lambda x: os.path.join(image_base_path, x))\n",
    "df = df.fillna(0)\n",
    "# Keep only the rows where the file exists.\n",
    "df = df[df['file_path'].apply(os.path.exists)]\n",
    "df['sex'] = df['gender'].map({'Male': 0, 'Female': 1})\n",
    "# Split into training and test sets based on the 'is_training' flag\n",
    "\n",
    "# from scipy.stats import boxcox\n",
    "\n",
    "# # Assume df is your DataFrame and it contains a 'bmi' column\n",
    "# # Check if BMI values are strictly positive. If not, shift them by adding a constant.\n",
    "# if (df['bmi'] <= 0).any():\n",
    "#     offset = 1 - df['bmi'].min()  # ensure minimum value becomes 1\n",
    "#     df['bmi_adjusted'] = df['bmi'] + offset\n",
    "# else:\n",
    "#     df['bmi_adjusted'] = df['bmi']\n",
    "\n",
    "# # Apply the Box-Cox transformation to the adjusted BMI column.\n",
    "# df['bmi_boxcox'], fitted_lambda = boxcox(df['bmi_adjusted'])\n",
    "\n",
    "# print(\"Fitted lambda for Box-Cox transformation:\", fitted_lambda)\n",
    "df_train_full = df[df['is_training'] == 1]\n",
    "df_test = df[df['is_training'] == 0]\n",
    "# Further split the full training set into train and validation sets (e.g., 80/20 split)\n",
    "df_train, df_valid = train_test_split(df_train_full, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bb0c14d-1c21-4f2b-bbc6-797f47e7ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def build_img_list(df, image_base_path):\n",
    "\n",
    "    # Returns a list of tuples: (full_image_path, bmi, sex)\n",
    "    # Assumes df contains columns: 'name', 'bmi', and 'sex'\n",
    "\n",
    "    return [\n",
    "        (os.path.join(image_base_path, row['name']), row['bmi'], row['sex'])\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "train_img_lst = build_img_list(df_train, image_base_path)\n",
    "valid_img_lst = build_img_list(df_valid, image_base_path)\n",
    "test_img_lst  = build_img_list(df_test, image_base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61345a6-3f78-4135-9a00-5cf68d6ee468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),              # Resize to 256x256\n",
    "    transforms.RandomCrop((224, 224)),            # Random crop to 224x224\n",
    "    transforms.RandomRotation(degrees=20),        # Random rotation (-20 to +20 degrees)\n",
    "    transforms.ToTensor(),                        # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),              # Resize to 256x256\n",
    "    transforms.CenterCrop((224, 224)),            # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ff1f25-d86b-448f-be95-6f3c00f0b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_list, transform=None):\n",
    "        \"\"\"\n",
    "        img_list: List of tuples (image_path, bmi, sex)\n",
    "        transform: Image transformations to apply\n",
    "        \"\"\"\n",
    "        self.img_list = img_list  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, bmi, sex = self.img_list[idx]\n",
    "        # Open the image and convert to RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Convert bmi and sex to tensors (adjust types as needed)\n",
    "        bmi = torch.tensor(bmi, dtype=torch.float32)\n",
    "        sex = torch.tensor(sex, dtype=torch.float32)\n",
    "        \n",
    "        # Return the image and its corresponding features/labels.\n",
    "        return image, bmi, sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc8b092-19c8-4877-87bb-04a5d82dd6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender</th>\n",
       "      <th>is_training</th>\n",
       "      <th>name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34.207396</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_0.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_0.bmp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26.453720</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>img_1.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_1.bmp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>34.967561</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_2.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_2.bmp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>22.044766</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_3.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_3.bmp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>25.845588</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>img_6.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_6.bmp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>4201</td>\n",
       "      <td>34.078947</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>img_4201.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_4201.bmp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>4202</td>\n",
       "      <td>34.564776</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>img_4202.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_4202.bmp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>4203</td>\n",
       "      <td>27.432362</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>img_4203.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_4203.bmp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>4204</td>\n",
       "      <td>40.492800</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>img_4204.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_4204.bmp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>4205</td>\n",
       "      <td>34.618844</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>img_4205.bmp</td>\n",
       "      <td>BMI/Data/Images\\img_4205.bmp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3962 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        bmi  gender  is_training          name  \\\n",
       "0              0  34.207396    Male            1     img_0.bmp   \n",
       "1              1  26.453720    Male            1     img_1.bmp   \n",
       "2              2  34.967561  Female            1     img_2.bmp   \n",
       "3              3  22.044766  Female            1     img_3.bmp   \n",
       "6              6  25.845588  Female            1     img_6.bmp   \n",
       "...          ...        ...     ...          ...           ...   \n",
       "4201        4201  34.078947    Male            0  img_4201.bmp   \n",
       "4202        4202  34.564776  Female            0  img_4202.bmp   \n",
       "4203        4203  27.432362  Female            0  img_4203.bmp   \n",
       "4204        4204  40.492800    Male            0  img_4204.bmp   \n",
       "4205        4205  34.618844    Male            0  img_4205.bmp   \n",
       "\n",
       "                         file_path  sex  \n",
       "0        BMI/Data/Images\\img_0.bmp    0  \n",
       "1        BMI/Data/Images\\img_1.bmp    0  \n",
       "2        BMI/Data/Images\\img_2.bmp    1  \n",
       "3        BMI/Data/Images\\img_3.bmp    1  \n",
       "6        BMI/Data/Images\\img_6.bmp    1  \n",
       "...                            ...  ...  \n",
       "4201  BMI/Data/Images\\img_4201.bmp    0  \n",
       "4202  BMI/Data/Images\\img_4202.bmp    1  \n",
       "4203  BMI/Data/Images\\img_4203.bmp    1  \n",
       "4204  BMI/Data/Images\\img_4204.bmp    0  \n",
       "4205  BMI/Data/Images\\img_4205.bmp    0  \n",
       "\n",
       "[3962 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc140b81-914e-49e6-b63c-e8d1c6136fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your datasets using your lists and appropriate transforms\n",
    "train_data = CustomImageDataset(train_img_lst, transform=train_transforms)\n",
    "valid_data = CustomImageDataset(valid_img_lst, transform=valid_transforms)\n",
    "test_data = CustomImageDataset(test_img_lst, transform=valid_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "713fc24c-c27d-4604-95ec-b5ff5d1deaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # Specified batch size is 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b738349a-a59a-43c0-831d-c397d4e91ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7cf981c-acfb-49a3-9936-15184054745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, load_weights=True, freeze_hidden_layers=False):\n",
    "        super(ResNet50, self).__init__()\n",
    "\n",
    "        # Load the pretrained ResNet50 model.\n",
    "        self.base_model = torchvision.models.resnet50(pretrained=load_weights)\n",
    "        \n",
    "        # Get the number of features from the original fc layer.\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        \n",
    "        # Remove the original fc layer.\n",
    "        self.base_model.fc = nn.Identity()\n",
    "        \n",
    "        # Create a new fc layer that accepts both the image features and the extra sex feature.\n",
    "        # We add one extra input feature to account for sex.\n",
    "        self.fc_reg = nn.Linear(num_ftrs + 1, 1)\n",
    "        \n",
    "        if freeze_hidden_layers:\n",
    "            for name, param in self.base_model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            # Optionally, allow gradients for the new fc_reg layer\n",
    "            for param in self.fc_reg.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x, sex):\n",
    "        # Get features from the base model.\n",
    "        features = self.base_model(x)\n",
    "        # Ensure the sex feature has shape [batch_size, 1]\n",
    "        if len(sex.shape) == 1:\n",
    "            sex = sex.unsqueeze(1)\n",
    "        # Concatenate the image features with the sex feature.\n",
    "        combined = torch.cat((features, sex), dim=1)\n",
    "        # Produce the final regression output.\n",
    "        output = self.fc_reg(combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41c4944-e79e-467c-83a7-c16a1dc8a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResNeXt101(nn.Module):\n",
    "    def __init__(self, load_weights=True, freeze_hidden_layers=False):\n",
    "        super(ResNeXt101, self).__init__()\n",
    "\n",
    "        # Load the pretrained ResNet50 model.\n",
    "        self.base_model = torchvision.models.resnext101_32x8d(pretrained=load_weights)\n",
    "        \n",
    "        # Get the number of features from the original fc layer.\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        \n",
    "        # Remove the original fc layer.\n",
    "        self.base_model.fc = nn.Identity()\n",
    "        \n",
    "        # Create a new fc layer that accepts both the image features and the extra feature.\n",
    "        self.fc_reg = nn.Linear(num_ftrs + 1, 1)\n",
    "        \n",
    "        if freeze_hidden_layers:\n",
    "            for name, param in self.base_model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            # Optionally, allow gradients for the new fc_reg layer\n",
    "            for param in self.fc_reg.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x, sex):\n",
    "        # Get features from the base model.\n",
    "        features = self.base_model(x)\n",
    "        # Ensure the sex feature has shape [batch_size, 1]\n",
    "        if len(sex.shape) == 1:\n",
    "            sex = sex.unsqueeze(1)\n",
    "        # Concatenate the image features with the sex feature.\n",
    "        combined = torch.cat((features, sex), dim=1)\n",
    "        # Produce the final regression output.\n",
    "        output = self.fc_reg(combined)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83d0bbfc-27c0-46c2-a60c-fe1a9b0d5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fb08b6e-a733-4a55-8c4f-dba33c13a0e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, roc_auc_score\n",
    "\n",
    "# Function to evaluate the model on a dataloader.\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X, y, sex in dataloader:\n",
    "            X, y, sex = X.to(device), y.to(device), sex.to(device)\n",
    "            preds = model(X, sex)\n",
    "            # Squeeze predictions from [batch_size, 1] to [batch_size]\n",
    "            preds = preds.squeeze(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "    return np.array(all_targets), np.array(all_preds)\n",
    "\n",
    "# Load your model. Ensure that the ResNet50 model definition matches the one used during training.\n",
    "# model = ResNet50()\n",
    "\n",
    "\n",
    "# # Evaluate the model on the test set.\n",
    "# y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# # Compute regression metrics.\n",
    "# mse = mean_squared_error(y_true, y_pred)\n",
    "# rmse = np.sqrt(mse)\n",
    "# mae = mean_absolute_error(y_true, y_pred)\n",
    "# mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "# print(\"Regression Metrics:\")\n",
    "# print(\"MSE:\", mse)\n",
    "# print(\"RMSE:\", rmse)\n",
    "# print(\"MAE:\", mae)\n",
    "# print(\"MAPE:\", mape)\n",
    "\n",
    "# # Optional: Calculate AUC if you have a binary classification task.\n",
    "# # For example, you can define BMI >= 25 as the positive class.\n",
    "# y_true_binary = (y_true >= 25).astype(int)\n",
    "# try:\n",
    "#     auc = roc_auc_score(y_true_binary, y_pred)\n",
    "#     print(\"AUC:\", auc)\n",
    "# except Exception as e:\n",
    "#     print(\"AUC calculation error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5ff30c2-9ec4-4a94-ab54-3d892c605c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chien\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\chien\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: 0.6610541843973373\n",
      "P-value: 1.2191729854467048e-95\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50()\n",
    "model.load_state_dict(torch.load('best_corr_resnet50_SGD_4.pth'))  # update file name if needed\n",
    "# model.load_state_dict(torch.load('best_lr_vggface2_Adam_0.pth'))  # update file name if needed\n",
    "model.to(device)\n",
    "from scipy.stats import pearsonr\n",
    "# model = ResNet50()\n",
    "# model.load_state_dict(torch.load('best_model_resnrt50_Adam_3.pth'))  # update file name if needed\n",
    "# model.to(device)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataloader and return the ground truth and predicted BMI values.\n",
    "    Assumes each batch from the dataloader returns a tuple: (X, y, sex)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X, y, sex in dataloader:\n",
    "            X, y, sex = X.to(device), y.to(device), sex.to(device)\n",
    "            preds = model(X, sex)\n",
    "            # Squeeze predictions from [batch_size, 1] to [batch_size]\n",
    "            preds = preds.squeeze(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "    return np.array(all_targets), np.array(all_preds)\n",
    "\n",
    "# Assuming 'model', 'test_loader', and 'device' are already defined and set up.\n",
    "# For example, load your model and move it to device:\n",
    "# model = ResNet50()\n",
    "# model.load_state_dict(torch.load('best_model_Q2_Adam_0.pth'))\n",
    "# model.to(device)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and its p-value.\n",
    "pearson_corr, p_value = pearsonr(y_true, y_pred)\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", pearson_corr)\n",
    "print(\"P-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff22440f-5a6e-44bd-b227-acee5fbbc813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Metrics:\n",
      "MSE: 51.35579\n",
      "RMSE: 7.1662955\n",
      "MAE: 5.026977\n",
      "MAPE: 0.14067928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# Compute regression metrics.\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MAPE:\", mape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
